Links
---
https://stackoverflow.com/questions/31669864/date-in-flask-url

https://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/
https://github.com/vinsci/geohash
https://www.coursera.org/lecture/deep-learning-in-computer-vision/computing-semantic-image-embeddings-using-convolutional-neural-networks-Oxtfc
https://www.tensorflow.org/tutorials/estimators/cnn
https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12
https://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/

Improvement Ideas
---

TODO: check how unbalanced the classes are.

Make it an ordinal regression task or normal regression task, instead of classification: does not make much sense as the evaluation metric is  multi-class logarithmic loss. 

- sensible default values for empty parameters (column values) e.g. average instead of 0. Mattrs for this algorithm
- make features of the words in the description. This can be easily done with tf-idf vectorizer from scikit, e.g. TfidfVectorizer(analyzer='word', ngram_range=(1,2), min_df=.01, max_df=.9, 
                                   lowercase=False, sublinear_tf=True, stop_words = stopwords_dunglish,
                                        preprocessor = clean_text)
 for going beyond simple local representations of words, we can use precomputed word2vec vectors.
- enable early stopping and use ypred = bst.predict(dtest, ntree_limit=bst.best_ntree_limit), see https://xgboost.readthedocs.io/en/latest/python/python_intro.html
- word2vec
- image hash as feature

-----

memory error when using full dataset...
----

Found myself:
doingparam combi 3 of 48
	num_boost_rounds: 100
	eta: 0.1
	gamma: 1
	max_depth: 5
	silent: 1
	
	
	high eta (0.5) not good
	low gamma <= 1 not good
	
----------

Not use only numerical features:

use categorical features :
	from "features"
	day_of_week
	from after processing "description"
	listing_id
	manager_id
	latitude, longitude to geohash
use word2vec embeddings:
	from description
use image embeddings:
	train deep CNN , use output / layer as feature?
		Not use pretrained NN, as these are not for houses.
		
		A deep convolutional neural network was chosen as the price predictor for this problem. This choice was motivated by the following considerations: First there is an abundant source of labelled data available for the task. We collected more than 100K labelled images for our training set, and more is available. Deep neural networks have been shown to achieve top performance against other models, given enough training data is available to train them and avoid overfitting. Hence sufficient data is crucially important as the network often contain millions of parameters, and regularization techniques such as dropout and data set augmentation so far lack substantive theoretical guarantees. Second is the ongoing promise of neural networks to eliminate the need for hand-engineered features. For the price regression problem, it is not at all clear which features from a photo are most important for price. Hence we forgo the problem of feature selection by collecting enough data for a deep convolutional network to automatically find the right features for our price prediction task.


Features
---

Integer features:
-
bathrooms: number of bathrooms 
bedrooms: number of bathrooms 
price: in USD 

is_weekday, is_sunday, hour, is_evening etc. from "created"



Geo features:
-
display_address 
street_address 

latitude 
longitude 

Free-text / categorical features:
-
description 
features: a list of features about this apartment 


Date features:
-
created 

Categorical features:
-
listing_id 
manager_id 
features: a list of features about this apartment 


Image features:
-
photos: a list of photo links. You are welcome to download the pictures yourselves from renthop's site, but they are the same as imgs.zip. 
